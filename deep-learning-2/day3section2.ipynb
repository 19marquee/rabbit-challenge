{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day3section2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRis+D8Vyoia0Fu2bDthLj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19marquee/rabbit-challenge/blob/main/deep-learning-2/day3section2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvTMNra91cL"
      },
      "source": [
        "# レポート 深層学習day3/Section2:LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zBbJNUdCNmD"
      },
      "source": [
        "## 100文字以内の要点のまとめ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqLDjSvkCN5A"
      },
      "source": [
        "- RNNの課題として時系列を遡れば遡るほど、勾配が消失するという問題がある。\n",
        "- 勾配消失：誤差逆伝播法が下位層に進んで行くに連れて、勾配がどんどん緩やかになっていく。そのため、勾配降下法による更新では、下位層のパラメータはほとんど変わらず、訓練は最適解に収束しなくなる。\n",
        "- LSTM：RNNモデルの構造を変えて勾配消失問題に対応。\n",
        "- 勾配爆発：勾配が層を逆伝播するごとに指数関数的に大きくなっていく。\n",
        "- CEC(Constant Error Carousel）：記憶機能だけを持つもの\n",
        "- CECの課題：入力データについて、時間依存に関係なく一律である。この課題を解決するために、入力ゲートと出力ゲートを追加する。\n",
        "- また、過去の状態がいらなくなった場合、そのタイミングで情報を忘却するために忘却ゲートを追加する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU616UZdD9Cs"
      },
      "source": [
        "##実装演習結果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "#### 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "#### Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a6e632-1cea-45c7-b45a-23c0cb1854e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "#### sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGmsHRwO-bi"
      },
      "source": [
        "#### simple RNN after\n",
        "#### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KNSG0aKXO-bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e30cc358-68df-4aa0-ae7f-a3e719e2a925"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:2.573022499129895\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "126 + 10 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.8524064946244583\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "0 + 115 = 255\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9406406070019078\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "40 + 46 = 223\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0944285493736217\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "124 + 98 = 131\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.9735424515105708\n",
            "Pred:[1 1 1 1 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "91 + 106 = 244\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0630068694201746\n",
            "Pred:[1 1 0 0 1 0 1 0]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "124 + 33 = 202\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0784997464583486\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "123 + 45 = 82\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.9711738897491676\n",
            "Pred:[0 0 1 1 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "28 + 77 = 58\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.1041832177537256\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "61 + 15 = 127\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0662915764666283\n",
            "Pred:[0 0 1 0 0 0 1 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "51 + 89 = 34\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.02077842480578\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "91 + 40 = 86\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.1231878590231625\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "41 + 7 = 63\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.9596387107789255\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "55 + 115 = 198\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.785162466962579\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "57 + 51 = 102\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.7605577620733969\n",
            "Pred:[1 1 1 0 0 0 1 0]\n",
            "True:[1 1 1 0 1 1 1 0]\n",
            "117 + 121 = 226\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.9057532768291129\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "70 + 88 = 136\n",
            "------------\n",
            "iters:1600\n",
            "Loss:1.0006242261749285\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "52 + 46 = 123\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.9331765085422202\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "123 + 32 = 222\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.838318976286397\n",
            "Pred:[1 1 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "115 + 55 = 239\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.685991291844198\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "14 + 85 = 107\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.9215424087003182\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "115 + 111 = 223\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.5193974632003422\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "34 + 94 = 128\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.469822588434527\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "0 + 91 = 90\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.4974863256864021\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "122 + 88 = 210\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.5672364646305978\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "95 + 10 = 104\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.5232330528000907\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "1 + 98 = 102\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.5474120875207\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "118 + 14 = 196\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.23623617698874289\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "1 + 69 = 71\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.22768041377020398\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "67 + 96 = 163\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.14295969836195566\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "37 + 85 = 122\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.2285314773636223\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "9 + 46 = 54\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.127145750639264\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "120 + 100 = 220\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.09361444387415054\n",
            "Pred:[0 0 1 0 0 0 1 0]\n",
            "True:[0 0 1 0 0 0 1 0]\n",
            "2 + 32 = 34\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.13377179932662792\n",
            "Pred:[1 1 1 0 0 0 1 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "120 + 106 = 226\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.1213171977623495\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "40 + 92 = 132\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.1040465513795187\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "77 + 41 = 118\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.16365496579995964\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "101 + 39 = 140\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.0871979917553556\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "20 + 88 = 108\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.08408562075779749\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "74 + 92 = 166\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.09874212533555617\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "11 + 70 = 81\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.07863081539599617\n",
            "Pred:[0 0 0 1 1 0 1 1]\n",
            "True:[0 0 0 1 1 0 1 1]\n",
            "23 + 4 = 27\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.16214818211398555\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "94 + 101 = 195\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.17808849656019404\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "125 + 11 = 136\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.06655596560842035\n",
            "Pred:[0 0 0 0 1 0 1 1]\n",
            "True:[0 0 0 0 1 0 1 1]\n",
            "3 + 8 = 11\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.10270337753584584\n",
            "Pred:[0 0 1 1 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "30 + 18 = 48\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.03410231744363436\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "62 + 63 = 125\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.0822910145944268\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "58 + 73 = 131\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.04477933138905758\n",
            "Pred:[1 1 1 1 0 1 1 0]\n",
            "True:[1 1 1 1 0 1 1 0]\n",
            "124 + 122 = 246\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.04293975006698501\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "87 + 53 = 140\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.0401422474253126\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "72 + 13 = 85\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.022925338907738714\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "76 + 125 = 201\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.029350529020164197\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "100 + 107 = 207\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.029504452975536208\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "113 + 29 = 142\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.03788032740705639\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "75 + 112 = 187\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.019847825328675663\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "41 + 71 = 112\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.0124486561594205\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "25 + 79 = 104\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.02941469408616132\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "38 + 123 = 161\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.026246883793391258\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "30 + 65 = 95\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.01059257053583766\n",
            "Pred:[1 1 1 1 1 0 0 0]\n",
            "True:[1 1 1 1 1 0 0 0]\n",
            "125 + 123 = 248\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.03966427349928582\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "70 + 50 = 120\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.014005881235207065\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "105 + 95 = 200\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.011460301964575483\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "93 + 63 = 156\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.020420219270789245\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "111 + 80 = 191\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.0054661597794039745\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "65 + 83 = 148\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.024715808950190172\n",
            "Pred:[0 0 1 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "19 + 40 = 59\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.010981527103497125\n",
            "Pred:[0 1 0 1 1 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "70 + 19 = 89\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.01763747154619911\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "29 + 74 = 103\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.02313463964589814\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "94 + 70 = 164\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.015057542275186887\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "127 + 32 = 159\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.007896435566744376\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "77 + 35 = 112\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.008703342029482287\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "77 + 19 = 96\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.00838866264017095\n",
            "Pred:[0 0 1 1 1 1 0 1]\n",
            "True:[0 0 1 1 1 1 0 1]\n",
            "56 + 5 = 61\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.012726137648287936\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "114 + 106 = 220\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.00761196424598609\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "92 + 53 = 145\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.013133993950464062\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "14 + 86 = 100\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.009352398458753035\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "110 + 53 = 163\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.007329838282479708\n",
            "Pred:[0 0 1 1 0 0 0 1]\n",
            "True:[0 0 1 1 0 0 0 1]\n",
            "40 + 9 = 49\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.013747560888729258\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "79 + 112 = 191\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.0025881384164027775\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "51 + 123 = 174\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0052107655730896036\n",
            "Pred:[1 1 1 1 0 0 0 1]\n",
            "True:[1 1 1 1 0 0 0 1]\n",
            "124 + 117 = 241\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.00800054594351768\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "120 + 32 = 152\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.005133241006510051\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "14 + 99 = 113\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0036560950201102614\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "11 + 33 = 44\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0063766777870967834\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "21 + 96 = 117\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.002761352426608849\n",
            "Pred:[1 1 1 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 0 0 0]\n",
            "101 + 123 = 224\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.006020326504379258\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "99 + 2 = 101\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.007056034774839682\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "0 + 54 = 54\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.009008761969302715\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "78 + 38 = 116\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.006847179435968304\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "12 + 120 = 132\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.005757450531316525\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "4 + 110 = 114\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.006083320665345468\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "28 + 78 = 106\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0033583035608702543\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "78 + 113 = 191\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.004142807658719132\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "43 + 101 = 144\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.004179604108650493\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "2 + 101 = 103\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0026493448443372387\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "8 + 99 = 107\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.004743679545629886\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "26 + 52 = 78\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.004338015862699186\n",
            "Pred:[0 1 0 0 1 0 0 1]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "37 + 36 = 73\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.004659463515395897\n",
            "Pred:[0 0 0 1 1 1 1 0]\n",
            "True:[0 0 0 1 1 1 1 0]\n",
            "22 + 8 = 30\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0023253902819158714\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "42 + 81 = 123\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0033271180978269544\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "39 + 57 = 96\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsBt7vxmO-bn"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#### [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "#### [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "#### [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obPPaBlGoOWm"
      },
      "source": [
        "重み初期値設定方法をHeに変更して学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0gTJVCsYoCBc",
        "outputId": "b0cd39a5-6e36-475b-eff0-9d3e479b118e"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.1498854077628455\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "59 + 123 = 0\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0822152840982948\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "51 + 97 = 255\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9500714718183145\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "59 + 10 = 0\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0594648626221506\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "103 + 77 = 1\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.1062390020194208\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "111 + 80 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.9771175504337098\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "125 + 42 = 251\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0279257145031628\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "32 + 111 = 65\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.9840411816891219\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "100 + 10 = 255\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.9574678534177812\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "4 + 66 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0067132151863933\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "121 + 63 = 0\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0294421695138247\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "125 + 47 = 0\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.012958876400906\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 1 0 0 1 0 1 1]\n",
            "125 + 78 = 176\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.017522524829422\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "27 + 93 = 38\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.9474406295142815\n",
            "Pred:[0 0 1 0 0 1 1 1]\n",
            "True:[0 0 0 1 0 1 1 1]\n",
            "19 + 4 = 39\n",
            "------------\n",
            "iters:1400\n",
            "Loss:1.1638117963716517\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "72 + 109 = 0\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.9620786998255952\n",
            "Pred:[0 0 0 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "70 + 38 = 8\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.926039745375587\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "47 + 83 = 0\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.9805869383835947\n",
            "Pred:[0 0 1 0 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "17 + 95 = 34\n",
            "------------\n",
            "iters:1800\n",
            "Loss:1.0589293749678335\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "31 + 108 = 127\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.8233839108129782\n",
            "Pred:[0 0 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "29 + 92 = 59\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.8972839236549701\n",
            "Pred:[1 1 1 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 1 1 1]\n",
            "119 + 112 = 224\n",
            "------------\n",
            "iters:2100\n",
            "Loss:1.0290541313401924\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "27 + 38 = 60\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.0303884665191299\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "42 + 121 = 208\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.9407231575908338\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "71 + 121 = 134\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.7987759048680338\n",
            "Pred:[0 0 0 0 1 1 0 0]\n",
            "True:[0 0 0 1 1 0 0 0]\n",
            "6 + 18 = 12\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.6161961946106251\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "68 + 100 = 136\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.0497212925652095\n",
            "Pred:[1 1 1 1 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "73 + 62 = 247\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.6123620296478826\n",
            "Pred:[0 0 0 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "37 + 5 = 10\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.78373872980088\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 0 1 0 1 0 0 1]\n",
            "1 + 40 = 1\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.5252562501100434\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "59 + 35 = 94\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.9175504527957055\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "63 + 116 = 251\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.76633266006258\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "26 + 33 = 63\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.27950215741533696\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "38 + 38 = 76\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.5698953424454575\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "98 + 118 = 221\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.5435790818246813\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "29 + 69 = 122\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.6071823109860582\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "56 + 66 = 123\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.5941500164190384\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "12 + 84 = 120\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.5397192519117218\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "41 + 19 = 44\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.4384934946122604\n",
            "Pred:[0 0 0 1 0 1 0 1]\n",
            "True:[0 0 0 1 0 0 1 1]\n",
            "9 + 10 = 21\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.5868199873927645\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "59 + 20 = 69\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.2637879061111674\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "57 + 116 = 173\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.34411697140913244\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "90 + 108 = 199\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.428363154921804\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "97 + 34 = 195\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.25568382145850166\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "105 + 29 = 134\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.30228064173371116\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "96 + 40 = 137\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.1424062141894411\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "105 + 93 = 198\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.09835001301633613\n",
            "Pred:[0 1 0 1 0 1 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "59 + 25 = 84\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.07195369186196061\n",
            "Pred:[0 0 1 0 0 1 0 1]\n",
            "True:[0 0 1 0 0 1 0 1]\n",
            "29 + 8 = 37\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.10000202746072923\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "31 + 39 = 70\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.6127149784407487\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "122 + 5 = 127\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.022782585825414762\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "75 + 31 = 106\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.03413337657485556\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "113 + 43 = 156\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.10775960508999279\n",
            "Pred:[1 1 0 0 1 0 1 1]\n",
            "True:[1 1 0 0 1 0 1 1]\n",
            "90 + 113 = 203\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.051548600345975304\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "12 + 83 = 95\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.010831076266994687\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "69 + 9 = 78\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.10440499837837136\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "107 + 18 = 125\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.04055217456397487\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "116 + 67 = 183\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.0756499844475629\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "66 + 40 = 106\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.07159965620183481\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "30 + 8 = 38\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.09040645235007848\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "46 + 22 = 68\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.05890599323217854\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "60 + 66 = 126\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.031996690945401314\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "49 + 94 = 143\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.027815667351989034\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "62 + 69 = 131\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.01804169315767523\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "107 + 42 = 149\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.05135021195829577\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "122 + 78 = 200\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.005283309157331372\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "115 + 17 = 132\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.027558449687786916\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "61 + 68 = 129\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.015633375810981044\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "32 + 107 = 139\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.046997055220056524\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "116 + 12 = 128\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.01623783789368074\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "40 + 13 = 53\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.014181999278766247\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "61 + 58 = 119\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.026865059209723464\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "124 + 30 = 154\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.013116521308432154\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "67 + 72 = 139\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.006533007156577072\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "15 + 76 = 91\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.008344373709073026\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "7 + 122 = 129\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.03315481385980222\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "22 + 118 = 140\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.007025178990434628\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "112 + 17 = 129\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.0074551993540925045\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "108 + 33 = 141\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.008853799995121458\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "2 + 113 = 115\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0009869429818385549\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "49 + 33 = 82\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.017726699318585493\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "48 + 34 = 82\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.015846209040816357\n",
            "Pred:[1 1 0 0 1 1 0 0]\n",
            "True:[1 1 0 0 1 1 0 0]\n",
            "96 + 108 = 204\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.015830065796366418\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "24 + 126 = 150\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.012716421701117427\n",
            "Pred:[0 0 1 1 1 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "56 + 0 = 56\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.004467359075572732\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "27 + 50 = 77\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.0011354888664333487\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "11 + 103 = 114\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0023182121613614286\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "99 + 63 = 162\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.012419114123365677\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "104 + 56 = 160\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.015031128691468462\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "10 + 126 = 136\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.002912540938792147\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "109 + 14 = 123\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0007549499095103405\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "75 + 89 = 164\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0005575537460579025\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "55 + 125 = 180\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.01070705696204531\n",
            "Pred:[1 0 1 1 1 1 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "122 + 66 = 188\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0025883454127933097\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "123 + 6 = 129\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.003385428194210987\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "14 + 71 = 85\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0035608461441633678\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "69 + 70 = 139\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.00230147105708434\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "118 + 43 = 161\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.003201321219199925\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "70 + 111 = 181\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.00045398339367897086\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "59 + 5 = 64\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0021417743596679655\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "33 + 84 = 117\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkerQIXwo_Ke"
      },
      "source": [
        "学習の進み具合は非安定的だが、lossが小さくなり学習が進んでいくことを確認。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqJtwzuDkNRk"
      },
      "source": [
        "##確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHCuswzag8Fj"
      },
      "source": [
        "・シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ。  \n",
        "(1) 0.15  \n",
        "(2) 0.25  \n",
        "(3) 0.35  \n",
        "(4) 0.45  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3L33mVZg9LD"
      },
      "source": [
        "解答  \n",
        "(1) 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NBgA8O9hfrR"
      },
      "source": [
        "・以下の文章をLSTMに入力し空欄に当てはまる単語を予測したいとする。\n",
        "文中の「とても」という言葉は空欄の予測においてなくなっても影響を及ぼさないと考えられる。  \n",
        "このような場合、どのゲートが作用すると考えられるか。  \n",
        "\n",
        "「映画おもしろかったね。ところで、とてもお腹が空いたから何か_____。」"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1YzA779hr26"
      },
      "source": [
        "解答  \n",
        "忘却ゲート"
      ]
    }
  ]
}