{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day3section7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMr9pkC3zAtUWvegJBJxLex",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19marquee/rabbit-challenge/blob/main/deep-learning-2/day3section7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvTMNra91cL"
      },
      "source": [
        "# レポート 深層学習day3/Section7:Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zBbJNUdCNmD"
      },
      "source": [
        "## 100文字以内の要点のまとめ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqLDjSvkCN5A"
      },
      "source": [
        "- Attention Mechanism：データ内もしくはデータ間の関連性に重みを付ける手法。自然言語処理では、1文の中で重要な単語を、モデル自身が見つける（注意を向ける）ように学習が行われる。seq2seqの長い文章への対応が難しいという課題に対して、重要な情報にだけ注意を向けて処理が行えるようにすることで対応。\n",
        "- 近年の精度の高い自然言語処理モデルには、全てAttention Mechanismが取り入れられているほど強力かつ注目をあびている手法である。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU616UZdD9Cs"
      },
      "source": [
        "##実装演習結果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjNjkn2rXGpu"
      },
      "source": [
        "該当なしのため、省略します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKCe0Z0r1FEl"
      },
      "source": [
        "##確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cfc-i7L1E9e"
      },
      "source": [
        "・RNNとWord2Vec、Seq2SeqとAttentionの違いを簡潔に述べよ。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acJh68u81TSP"
      },
      "source": [
        "解答  \n",
        "\n",
        "RNNは時系列データを処理、扱うのに適したネットワーク。  \n",
        "Word2Vecは単語の分散表現を得る手法。  \n",
        "Seq2Seqは、時系列データを別の時系列データに変換するネットワーク。  \n",
        "Attentionはデータ内もしくはデータ間の関連性に重みを付ける手法。  "
      ]
    }
  ]
}