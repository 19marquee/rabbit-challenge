{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day4section1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPeidfFEeGwskmEmh1aaeGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19marquee/rabbit-challenge/blob/main/deep-learning-2/day4section1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvTMNra91cL"
      },
      "source": [
        "# レポート 深層学習day4/Section1:強化学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zBbJNUdCNmD"
      },
      "source": [
        "## 100文字以内の要点のまとめ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqLDjSvkCN5A"
      },
      "source": [
        "- 強化学習：行動の結果として与えられる利益（報酬）をもとに行動を決定する原理/規則を獲得/改善していく仕組み。機械学習手法の1種。\n",
        "- 環境という与えられた枠組みの中で行動を選択するエージェントと環境の相互作用。長期的にエージェントが得られる報酬を最大化するような行動をとるように学習が行われる。\n",
        "- 強化学習は、良い方策（戦略）を見つけることが目標となる。対して、教師あり学習、教師なし学習は、データから特徴量を見つけ出し、未知のデータを予想する。\n",
        "- 探索と利用のトレードオフ：不完全な知識をもとに行動しながらデータを収集しながら、最適な行動を見つけていく。\n",
        "- 強化学習で具体的に学習するものは、方策関数、行動価値関数である。\n",
        "- 方策関数：方策ベースの強化学習手法において、ある状態でどのような行動を採るのかの確率を与える関数。\n",
        "- 状態価値関数：ある状態の価値に注目する。\n",
        "- 行動価値関数：状態と価値を組み合わせた価値に注目する。\n",
        "- 計算速度の進展により大規模な状態を持つ場合での学習が可能となりつつある。\n",
        "- 関数近似法：価値関数や方策関数を関数近似する手法。\n",
        "- Ｑ学習：行動価値関数を、行動する度に更新していきながら学習を進める手法。\n",
        "- 方策反復法：方策をモデル化して最適化する手法で、これに勾配を用いたものを方策勾配法という。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU616UZdD9Cs"
      },
      "source": [
        "##実装演習結果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjNjkn2rXGpu"
      },
      "source": [
        "該当なしのため、省略します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKCe0Z0r1FEl"
      },
      "source": [
        "##確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtgkmG7MlNjU"
      },
      "source": [
        "該当なしのため、省略します。"
      ]
    }
  ]
}