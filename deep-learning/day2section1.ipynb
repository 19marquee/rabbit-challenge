{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day2section1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMssX9kcwtrv7MNFgRRGSHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19marquee/rabbit-challenge/blob/main/deep-learning/day2section1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvTMNra91cL"
      },
      "source": [
        "# レポート 深層学習day2/Section1:勾配消失問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zBbJNUdCNmD"
      },
      "source": [
        "## 100文字以内の要点のまとめ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqLDjSvkCN5A"
      },
      "source": [
        "- 誤差が下位に進んでいくに連れ、勾配が緩やかになっていく。\n",
        "- 勾配消失問題：連鎖率を用いて微分値がかけられていくと、その値は0以上1以下の範囲であるため、勾配は小さくなっていく。\n",
        "- シグモイド関数の微分の最大値は0.25。勾配消失問題を引き起こしやすい。対策としては、Relu関数に変える。\n",
        "- 重みの初期値設定方法として、Xavierがある。重みの要素を、前の層のノード数の平方根で除算した値となる。活性化関数の表現力を保ったまま、勾配消失の対策ができる。\n",
        "- バッチ正規化：ミニバッチ単位で、入力値のデータの偏りを減らす手法。学習の速度向上、過学習の防止の効果がある。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-vKleZObFGp"
      },
      "source": [
        "## 実装演習のファイル量、コード量が多いため、実装演習結果は、別ファイルに記載します"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWY8egTgbInr"
      },
      "source": [
        "・day2section1_1.ipynb  \n",
        "・day2section1_2.ipynb  \n",
        "・day2section1_3.ipynb  \n",
        "・day2section1_4.ipynb  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LM1RxXI5Es_"
      },
      "source": [
        "##確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJmuWIbC5JR0"
      },
      "source": [
        "・連鎖率の原理を使い、dz/dx を求めよ。\n",
        "$$\n",
        "z = t^2\n",
        "$$\n",
        "$$\n",
        "t = x + y\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_3sXxcr5ZlP"
      },
      "source": [
        "$\\dfrac{dz}{dt} = 2t$、$\\dfrac{dt}{dx} = 1$より、\n",
        "$\\dfrac{dz}{dx} = \\dfrac{dz}{dt}\\dfrac{dt}{dx} = 2t * 1 = 2t = 2(x+y)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oON156Ne5MwQ"
      },
      "source": [
        "・シグモイド関数を微分した時、入力値が０の時に最大値をとる。その値として正しいものを選択肢から選べ。  \n",
        "(1) 0.15  \n",
        "(2) 0.25  \n",
        "(3) 0.35  \n",
        "(4) 0.45  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-HnIYgq5Z_B"
      },
      "source": [
        "解答  \n",
        " (2) 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3cuLJ785QpW"
      },
      "source": [
        "重みの初期値に０を設定すると、どのような問題が発生するか。簡潔に説明せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-BcOBVw5avI"
      },
      "source": [
        "解答  \n",
        "重みを０で初期化すると正しい学習が行えなくなる。 全ての重みの値が均一に更新されるため、多数の重みを持つ意味がなくなり、学習が上手くいかない。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg6kceqa5Q4x"
      },
      "source": [
        "・一般的に考えられるバッチ正規化の効果を２点挙げよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQf6Qm0z5bUk"
      },
      "source": [
        "解答  \n",
        "ニューラルネットワークの学習時の中間層の重みの更新が安定化する。その結果として学習速度が上がる。  \n",
        "過学習を押さえる事ができる。学習データの極端なばらつきがない状態でニューラルネットワークを学習できるため、過学習が起きにくい。"
      ]
    }
  ]
}