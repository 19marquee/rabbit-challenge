{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day1section3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPj6iTUhaDDULu3FstqLx4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19marquee/rabbit-challenge/blob/main/deep-learning/day1section3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvTMNra91cL"
      },
      "source": [
        "# レポート 深層学習day1/Section3:出力層\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zBbJNUdCNmD"
      },
      "source": [
        "## 100文字以内の要点のまとめ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqLDjSvkCN5A"
      },
      "source": [
        "- 誤差関数：出力と訓練データの誤差を算出する。\n",
        "- 代表的な誤差関数として、二乗和誤差がある。\n",
        "- 分類問題の場合、誤差関数にクロスエントロピー誤差を用いることが一般的である。\n",
        "- 中間層と出力層で活性化関数を使う目的が異なる。中間層は、閾値の前後で信号の強弱を調整、一方出力層は、信号の大きさ(比率)はそのままで変換。その為利用する活性化関数が異なる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU616UZdD9Cs"
      },
      "source": [
        "##実装演習結果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "#### 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "#### Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f77a05e-d3ae-41e6-d05d-3ff84a2827ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "#### sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5L0AWliZYng"
      },
      "source": [
        "#### importと関数定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNcuxoGogLvM"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def print_vec(text, vec):\n",
        "    print(\"*** \" + text + \" ***\")\n",
        "    print(vec)\n",
        "    #print(\"shape: \" + str(x.shape))\n",
        "    print(\"\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhkhpJjOgbtF"
      },
      "source": [
        "#### メインプログラム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma1nAxuegLvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92470c78-610f-48b6-a360-ef0974b6dfde"
      },
      "source": [
        "# ウェイトとバイアスを設定\n",
        "# ネートワークを作成\n",
        "def init_network():\n",
        "    print(\"##### ネットワークの初期化 #####\")\n",
        "\n",
        "    network = {}\n",
        "    network['W1'] = np.array([\n",
        "        [0.1, 0.3, 0.5],\n",
        "        [0.2, 0.4, 0.6]\n",
        "    ])\n",
        "\n",
        "    network['W2'] = np.array([\n",
        "        [0.1, 0.4],\n",
        "        [0.2, 0.5],\n",
        "        [0.3, 0.6]\n",
        "    ])\n",
        "\n",
        "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
        "    network['b2'] = np.array([0.1, 0.2])\n",
        "    \n",
        "    print_vec(\"重み1\", network['W1'])\n",
        "    print_vec(\"重み2\", network['W2'])\n",
        "    print_vec(\"バイアス1\", network['b1'])\n",
        "    print_vec(\"バイアス2\", network['b2'])\n",
        "\n",
        "    return network\n",
        "\n",
        "# 順伝播\n",
        "def forward(network, x):\n",
        "    print(\"##### 順伝播開始 #####\")\n",
        "\n",
        "    W1, W2 = network['W1'], network['W2']\n",
        "    b1, b2 = network['b1'], network['b2']\n",
        "    \n",
        "    u1 = np.dot(x, W1) + b1\n",
        "    z1 = functions.relu(u1)\n",
        "    u2 = np.dot(z1, W2) + b2\n",
        "    y = functions.softmax(u2)\n",
        "    \n",
        "    print_vec(\"総入力1\", u1)\n",
        "    print_vec(\"中間層出力1\", z1)\n",
        "    print_vec(\"総入力2\", u2)\n",
        "    print_vec(\"出力1\", y)\n",
        "    print(\"出力合計: \" + str(np.sum(y)))\n",
        "\n",
        "    return y, z1\n",
        "\n",
        "# 誤差逆伝播\n",
        "def backward(x, d, z1, y):\n",
        "    print(\"\\n##### 誤差逆伝播開始 #####\")\n",
        "\n",
        "    grad = {}\n",
        "\n",
        "    W1, W2 = network['W1'], network['W2']\n",
        "    b1, b2 = network['b1'], network['b2']\n",
        "    #  出力層でのデルタ\n",
        "    delta2 = functions.d_sigmoid_with_loss(d, y)\n",
        "    #  b2の勾配\n",
        "    grad['b2'] = np.sum(delta2, axis=0)\n",
        "    #  W2の勾配\n",
        "    grad['W2'] = np.dot(z1.T, delta2)\n",
        "    #  中間層でのデルタ\n",
        "    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)\n",
        "    # b1の勾配\n",
        "    grad['b1'] = np.sum(delta1, axis=0)\n",
        "    #  W1の勾配\n",
        "    grad['W1'] = np.dot(x.T, delta1)\n",
        "        \n",
        "    print_vec(\"偏微分_dE/du2\", delta2)\n",
        "    print_vec(\"偏微分_dE/du2\", delta1)\n",
        "\n",
        "    print_vec(\"偏微分_重み1\", grad[\"W1\"])\n",
        "    print_vec(\"偏微分_重み2\", grad[\"W2\"])\n",
        "    print_vec(\"偏微分_バイアス1\", grad[\"b1\"])\n",
        "    print_vec(\"偏微分_バイアス2\", grad[\"b2\"])\n",
        "\n",
        "    return grad\n",
        "    \n",
        "# 訓練データ\n",
        "x = np.array([[1.0, 5.0]])\n",
        "# 目標出力\n",
        "d = np.array([[0, 1]])\n",
        "#  学習率\n",
        "learning_rate = 0.01\n",
        "network =  init_network()\n",
        "y, z1 = forward(network, x)\n",
        "\n",
        "# 誤差\n",
        "loss = functions.cross_entropy_error(d, y)\n",
        "\n",
        "grad = backward(x, d, z1, y)\n",
        "for key in ('W1', 'W2', 'b1', 'b2'):\n",
        "    network[key]  -= learning_rate * grad[key]\n",
        "\n",
        "print(\"##### 結果表示 #####\")    \n",
        "\n",
        "\n",
        "print(\"##### 更新後パラメータ #####\") \n",
        "print_vec(\"重み1\", network['W1'])\n",
        "print_vec(\"重み2\", network['W2'])\n",
        "print_vec(\"バイアス1\", network['b1'])\n",
        "print_vec(\"バイアス2\", network['b2'])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##### ネットワークの初期化 #####\n",
            "*** 重み1 ***\n",
            "[[0.1 0.3 0.5]\n",
            " [0.2 0.4 0.6]]\n",
            "\n",
            "*** 重み2 ***\n",
            "[[0.1 0.4]\n",
            " [0.2 0.5]\n",
            " [0.3 0.6]]\n",
            "\n",
            "*** バイアス1 ***\n",
            "[0.1 0.2 0.3]\n",
            "\n",
            "*** バイアス2 ***\n",
            "[0.1 0.2]\n",
            "\n",
            "##### 順伝播開始 #####\n",
            "*** 総入力1 ***\n",
            "[[1.2 2.5 3.8]]\n",
            "\n",
            "*** 中間層出力1 ***\n",
            "[[1.2 2.5 3.8]]\n",
            "\n",
            "*** 総入力2 ***\n",
            "[[1.86 4.21]]\n",
            "\n",
            "*** 出力1 ***\n",
            "[[0.08706577 0.91293423]]\n",
            "\n",
            "出力合計: 1.0\n",
            "\n",
            "##### 誤差逆伝播開始 #####\n",
            "*** 偏微分_dE/du2 ***\n",
            "[[ 0.08706577 -0.08706577]]\n",
            "\n",
            "*** 偏微分_dE/du2 ***\n",
            "[[-0.02611973 -0.02611973 -0.02611973]]\n",
            "\n",
            "*** 偏微分_重み1 ***\n",
            "[[-0.02611973 -0.02611973 -0.02611973]\n",
            " [-0.13059866 -0.13059866 -0.13059866]]\n",
            "\n",
            "*** 偏微分_重み2 ***\n",
            "[[ 0.10447893 -0.10447893]\n",
            " [ 0.21766443 -0.21766443]\n",
            " [ 0.33084994 -0.33084994]]\n",
            "\n",
            "*** 偏微分_バイアス1 ***\n",
            "[-0.02611973 -0.02611973 -0.02611973]\n",
            "\n",
            "*** 偏微分_バイアス2 ***\n",
            "[ 0.08706577 -0.08706577]\n",
            "\n",
            "##### 結果表示 #####\n",
            "##### 更新後パラメータ #####\n",
            "*** 重み1 ***\n",
            "[[0.1002612  0.3002612  0.5002612 ]\n",
            " [0.20130599 0.40130599 0.60130599]]\n",
            "\n",
            "*** 重み2 ***\n",
            "[[0.09895521 0.40104479]\n",
            " [0.19782336 0.50217664]\n",
            " [0.2966915  0.6033085 ]]\n",
            "\n",
            "*** バイアス1 ***\n",
            "[0.1002612 0.2002612 0.3002612]\n",
            "\n",
            "*** バイアス2 ***\n",
            "[0.09912934 0.20087066]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqJtwzuDkNRk"
      },
      "source": [
        "##確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ZESy2D7-_U"
      },
      "source": [
        "・⑴なぜ、引き算でなく二乗するか述べよ  \n",
        "・⑵下式の1/2はどういう意味を持つか述べよ  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOYQEcH1_2Mo"
      },
      "source": [
        "解答  \n",
        "⑴引き算だけでは、誤差として正負両方の値をとるようになり、全体の誤差を正しく表すのに都合が悪い。そのため、2乗することで誤差を正の値にして、値が大きいほど誤差が大きくなるように設計する。  \n",
        "⑵本質的な意味、理由はなく、実際の学習時に行う誤差逆伝搬の計算で、誤差関数を微分をする際の計算を簡単にするために1/2をつける。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa_ZBrrR8FXq"
      },
      "source": [
        "・以下の数式に該当するソースコードを示し、一行づつ処理の説明をせよ。\n",
        "$$\n",
        "f(i,u) = \\dfrac{e^{u_i}}{\\displaystyle{\\Sigma_{k=1}^n} e^{u_k}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INEf-KmIBmc6"
      },
      "source": [
        "解答  \n",
        "(コード内の###に記載)  \n",
        "```\n",
        "# 出力層の活性化関数\n",
        "# ソフトマックス関数\n",
        "def softmax(x):\n",
        "    if x.ndim == 2: ###if文の中の処理は、ミニバッチ学習を行う際に必要な処理\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # オーバーフロー対策 ###計算を行ううえで、プログラムを安定させるために必要\n",
        "    return np.exp(x) / np.sum(np.exp(x)) ###数式を表す、本質的な部分\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0T0wOZ88FcT"
      },
      "source": [
        "・以下の数式に該当するソースコードを示し、一行づつ処理の説明をせよ。\n",
        "$$\n",
        "E_n(W) = -\\displaystyle{\\Sigma_{i=1}^l d_i \\log y_i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcctyBIGCAge"
      },
      "source": [
        "解答  \n",
        "(コード内の###に記載)  \n",
        "\n",
        "```\n",
        "# クロスエントロピー\n",
        "def cross_entropy_error(d, y):\n",
        "    if y.ndim == 1: ###ミニバッチのサイズが1として解釈する処理\n",
        "        d = d.reshape(1, d.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "    if d.size == y.size:\n",
        "        d = d.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size ###-np.sum(np.log(y[np.arange(batch_size), d] + 1e-7))が、数式を表す本質的な部分\n",
        "    ###対数関数は０に近づくと－∞に落ちる。それを避けるために小さな値（1e-7）を付与している。\n",
        "    ###yには０と１が並んでいる（正解1、不正解0)。\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}